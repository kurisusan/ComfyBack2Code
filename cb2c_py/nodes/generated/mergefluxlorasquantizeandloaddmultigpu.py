
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node

class MergeFluxLoRAsQuantizeAndLoaddMultiGPU(Node):
    """
    Original name: MergeFluxLoRAsQuantizeAndLoaddMultiGPU
    No description available.
    """
    _inputs = {
    "unet_name": [
        [
            "Wan2_1-VACE_module_14B_fp8_e4m3fn.safetensors",
            "Wan2_1_VACE_1_3B_preview_bf16.safetensors",
            "cosmos_predict2_2B_video2world_480p_16fps.safetensors",
            "omnigen2_fp16.safetensors",
            "svdq-fp4_r32-flux.1-kontext-dev.safetensors",
            "wan2.1_vace_1.3B_fp16.safetensors",
            "wan2.1_vace_14B_fp8_e4m3fn.safetensors"
        ]
    ],
    "switch_1": [
        [
            "Off",
            "On"
        ]
    ],
    "lora_name_1": [
        [
            "None",
            "MoXinV1.safetensors",
            "Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
            "Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
            "blindbox_v1_mix.safetensors"
        ]
    ],
    "lora_weight_1": [
        "FLOAT",
        {
            "default": 1.0,
            "min": -10.0,
            "max": 10.0,
            "step": 0.01
        }
    ],
    "switch_2": [
        [
            "Off",
            "On"
        ]
    ],
    "lora_name_2": [
        [
            "None",
            "MoXinV1.safetensors",
            "Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
            "Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
            "blindbox_v1_mix.safetensors"
        ]
    ],
    "lora_weight_2": [
        "FLOAT",
        {
            "default": 1.0,
            "min": -10.0,
            "max": 10.0,
            "step": 0.01
        }
    ],
    "switch_3": [
        [
            "Off",
            "On"
        ]
    ],
    "lora_name_3": [
        [
            "None",
            "MoXinV1.safetensors",
            "Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
            "Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
            "blindbox_v1_mix.safetensors"
        ]
    ],
    "lora_weight_3": [
        "FLOAT",
        {
            "default": 1.0,
            "min": -10.0,
            "max": 10.0,
            "step": 0.01
        }
    ],
    "switch_4": [
        [
            "Off",
            "On"
        ]
    ],
    "lora_name_4": [
        [
            "None",
            "MoXinV1.safetensors",
            "Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
            "Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
            "blindbox_v1_mix.safetensors"
        ]
    ],
    "lora_weight_4": [
        "FLOAT",
        {
            "default": 1.0,
            "min": -10.0,
            "max": 10.0,
            "step": 0.01
        }
    ],
    "quantization": [
        [
            "Q2_K",
            "Q3_K_S",
            "Q4_0",
            "Q4_1",
            "Q4_K_S",
            "Q5_0",
            "Q5_1",
            "Q5_K_S",
            "Q6_K",
            "Q8_0",
            "FP16"
        ],
        {
            "default": "Q4_K_S"
        }
    ],
    "delete_final_gguf": [
        "BOOLEAN",
        {
            "default": False
        }
    ],
    "new_model_name": [
        "STRING",
        {
            "default": "merged_model"
        }
    ]
}
    _outputs = [
    "MODEL"
]
    _original_name = "MergeFluxLoRAsQuantizeAndLoaddMultiGPU"

    def __init__(self, unet_name, switch_1, lora_name_1, switch_2, lora_name_2, switch_3, lora_name_3, switch_4, lora_name_4, lora_weight_1=1.0, lora_weight_2=1.0, lora_weight_3=1.0, lora_weight_4=1.0, quantization="Q4_K_S", delete_final_gguf=False, new_model_name="merged_model"):
        super().__init__(unet_name=unet_name, switch_1=switch_1, lora_name_1=lora_name_1, lora_weight_1=lora_weight_1, switch_2=switch_2, lora_name_2=lora_name_2, lora_weight_2=lora_weight_2, switch_3=switch_3, lora_name_3=lora_name_3, lora_weight_3=lora_weight_3, switch_4=switch_4, lora_name_4=lora_name_4, lora_weight_4=lora_weight_4, quantization=quantization, delete_final_gguf=delete_final_gguf, new_model_name=new_model_name)

    @classmethod
    def get_inputs(cls):
        return cls._inputs

    @classmethod
    def get_outputs(cls):
        return cls._outputs
