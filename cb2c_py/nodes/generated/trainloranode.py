
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node

class TrainLoraNode(Node):
    """
    Original name: TrainLoraNode
    No description available.
    """
    _inputs = {
    "model": [
        "MODEL",
        {
            "tooltip": "The model to train the LoRA on."
        }
    ],
    "latents": [
        "LATENT",
        {
            "tooltip": "The Latents to use for training, serve as dataset/input of the model."
        }
    ],
    "positive": [
        "CONDITIONING",
        {
            "tooltip": "The positive conditioning to use for training."
        }
    ],
    "batch_size": [
        "INT",
        {
            "default": 1,
            "min": 1,
            "max": 10000,
            "step": 1,
            "tooltip": "The batch size to use for training."
        }
    ],
    "steps": [
        "INT",
        {
            "default": 16,
            "min": 1,
            "max": 100000,
            "tooltip": "The number of steps to train the LoRA for."
        }
    ],
    "learning_rate": [
        "FLOAT",
        {
            "default": 0.0005,
            "min": 1e-07,
            "max": 1.0,
            "step": 1e-06,
            "tooltip": "The learning rate to use for training."
        }
    ],
    "rank": [
        "INT",
        {
            "default": 8,
            "min": 1,
            "max": 128,
            "tooltip": "The rank of the LoRA layers."
        }
    ],
    "optimizer": [
        [
            "AdamW",
            "Adam",
            "SGD",
            "RMSprop"
        ],
        {
            "default": "AdamW",
            "tooltip": "The optimizer to use for training."
        }
    ],
    "loss_function": [
        [
            "MSE",
            "L1",
            "Huber",
            "SmoothL1"
        ],
        {
            "default": "MSE",
            "tooltip": "The loss function to use for training."
        }
    ],
    "seed": [
        "INT",
        {
            "default": 0,
            "min": 0,
            "max": 18446744073709551615,
            "tooltip": "The seed to use for training (used in generator for LoRA weight initialization and noise sampling)"
        }
    ],
    "training_dtype": [
        [
            "bf16",
            "fp32"
        ],
        {
            "default": "bf16",
            "tooltip": "The dtype to use for training."
        }
    ],
    "lora_dtype": [
        [
            "bf16",
            "fp32"
        ],
        {
            "default": "bf16",
            "tooltip": "The dtype to use for lora."
        }
    ],
    "existing_lora": [
        [
            "MoXinV1.safetensors",
            "Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
            "Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
            "blindbox_v1_mix.safetensors",
            "[None]"
        ],
        {
            "default": "[None]",
            "tooltip": "The existing LoRA to append to. Set to None for new LoRA."
        }
    ]
}
    _outputs = [
    "MODEL",
    "LORA_MODEL",
    "LOSS_MAP",
    "INT"
]
    _original_name = "TrainLoraNode"

    def __init__(self, model, latents, positive, batch_size=1, steps=16, learning_rate=0.0005, rank=8, optimizer="AdamW", loss_function="MSE", seed=0, training_dtype="bf16", lora_dtype="bf16", existing_lora="[None]"):
        super().__init__(model=model, latents=latents, positive=positive, batch_size=batch_size, steps=steps, learning_rate=learning_rate, rank=rank, optimizer=optimizer, loss_function=loss_function, seed=seed, training_dtype=training_dtype, lora_dtype=lora_dtype, existing_lora=existing_lora)

    @classmethod
    def get_inputs(cls):
        return cls._inputs

    @classmethod
    def get_outputs(cls):
        return cls._outputs
