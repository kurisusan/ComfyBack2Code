
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node, InputSlots, OutputSlots, Slot, Model, Conditioning, Latent, Image, Vae, Clip
from typing import Dict, Any, List, Union

# Define input and output slot classes for PatchModelPatcherOrder
class PatchModelPatcherOrderInputs(InputSlots):
    model: Slot[Model]
    patch_order: Slot[str]
    full_load: Slot[str]
    def __init__(self, node: "Node"):
        self.model = Slot[Model](node, "model", 'MODEL')
        self.patch_order = Slot[str](node, "patch_order", ['object_patch_first', 'weight_patch_first'])
        self.full_load = Slot[str](node, "full_load", ['enabled', 'disabled', 'auto'])

class PatchModelPatcherOrderOutputs(OutputSlots):
    model: Slot[Model]
    def __init__(self, node: "Node"):
        self.model = Slot[Model](node, "MODEL", 'MODEL')

class PatchModelPatcherOrder(Node[PatchModelPatcherOrderInputs, PatchModelPatcherOrderOutputs]):
    """
    Original name: PatchModelPatcherOrder
    Category: KJNodes/experimental
    Patch the comfy patch_model function patching order, useful for torch.compile (used as object_patch) as it should come last if you want to use LoRAs with compile

    Inputs:
        - model (Model)
        - patch_order (str) (default: 'weight_patch_first')
          Patch the comfy patch_model function to load weight patches (LoRAs) before compiling the model
        - full_load (str) (default: 'auto')
          Disabling may help with memory issues when loading large models, when changing this you should probably force model reload to avoid issues!

    Outputs:
        - model (Model)
    """
    _original_name: str = 'PatchModelPatcherOrder'

    def __init__(self, model: Slot[Model], patch_order: str = 'weight_patch_first', full_load: str = 'auto'):
        super().__init__(**{"model": model, "patch_order": patch_order, "full_load": full_load})
        self.inputs = PatchModelPatcherOrderInputs(self)
        self.outputs = PatchModelPatcherOrderOutputs(self)
