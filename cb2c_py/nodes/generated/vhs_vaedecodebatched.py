
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node, InputSlots, OutputSlots, Slot, Model, Conditioning, Latent, Image, Vae, Clip
from typing import Dict, Any, List, Union

# Define input and output slot classes for VHS_VAEDecodeBatched
class VHS_VAEDecodeBatchedInputs(InputSlots):
    samples: Slot[Latent]
    vae: Slot[Vae]
    per_batch: Slot[int]
    def __init__(self, node: "Node"):
        self.samples = Slot[Latent](node, "samples", 'LATENT')
        self.vae = Slot[Vae](node, "vae", 'VAE')
        self.per_batch = Slot[int](node, "per_batch", 'INT')

class VHS_VAEDecodeBatchedOutputs(OutputSlots):
    image: Slot[Image]
    def __init__(self, node: "Node"):
        self.image = Slot[Image](node, "IMAGE", 'IMAGE')

class VHS_VAEDecodeBatched(Node[VHS_VAEDecodeBatchedInputs, VHS_VAEDecodeBatchedOutputs]):
    """
    Original name: VHS_VAEDecodeBatched
    Category: Video Helper Suite ðŸŽ¥ðŸ…¥ðŸ…—ðŸ…¢/batched nodes
    VAE Decode Batched ðŸŽ¥ðŸ…¥ðŸ…—ðŸ…¢<div style="font-size: 0.8em"><div id=VHS_shortdesc>Decode latents to images with a manually specified batch size</div></div><div style="font-size: 0.8em">Some people have ran into VRAM issues when encoding or decoding large batches of images. As a workaround, this node lets you manually set a batch size when decoding latents.</div><div style="font-size: 0.8em">Unless these issues have been encountered, it is simpler to use the native VAE Decode or to decode from a Video Combine directly</div><div style="font-size: 0.8em"><div vhs_title="Inputs" style="display: flex; font-size: 0.8em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">Inputs: <div vhs_title="samples" style="display: flex; font-size: 1em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">samples: The latents to be decoded.</div></div><div vhs_title="vae" style="display: flex; font-size: 1em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">vae: The VAE to use when decoding.</div></div></div></div><div vhs_title="Outputs" style="display: flex; font-size: 0.8em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">Outputs: <div vhs_title="IMAGE" style="display: flex; font-size: 1em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">IMAGE: The decoded images.</div></div></div></div><div vhs_title="Widgets" style="display: flex; font-size: 0.8em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">Widgets: <div vhs_title="per_batch" style="display: flex; font-size: 1em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">per_batch: The maximum number of images to decode in each batch.</div></div></div></div></div>

    Inputs:
        - samples (Latent)
        - vae (Vae)
        - per_batch (int) (default: 16)

    Outputs:
        - image (Image)
    """
    _original_name: str = 'VHS_VAEDecodeBatched'

    def __init__(self, samples: Slot[Latent], vae: Slot[Vae], per_batch: int = 16):
        super().__init__(**{"samples": samples, "vae": vae, "per_batch": per_batch})
        self.inputs = VHS_VAEDecodeBatchedInputs(self)
        self.outputs = VHS_VAEDecodeBatchedOutputs(self)
