
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node, InputSlots, OutputSlots, Slot, Model, Conditioning, Latent, Image, Vae, Clip
from typing import Dict, Any, List, Union

# Define input and output slot classes for NunchakuFluxDiTLoader
class NunchakuFluxDiTLoaderInputs(InputSlots):
    model_path: Slot[str]
    cache_threshold: Slot[Union[float, int]]
    attention: Slot[str]
    cpu_offload: Slot[str]
    device_id: Slot[int]
    data_type: Slot[str]
    def __init__(self, node: "Node"):
        self.model_path = Slot[str](node, "model_path", [])
        self.cache_threshold = Slot[Union[float, int]](node, "cache_threshold", 'FLOAT')
        self.attention = Slot[str](node, "attention", ['nunchaku-fp16', 'flash-attention2'])
        self.cpu_offload = Slot[str](node, "cpu_offload", ['auto', 'enable', 'disable'])
        self.device_id = Slot[int](node, "device_id", 'INT')
        self.data_type = Slot[str](node, "data_type", ['bfloat16', 'float16'])

class NunchakuFluxDiTLoaderOutputs(OutputSlots):
    model: Slot[Model]
    def __init__(self, node: "Node"):
        self.model = Slot[Model](node, "MODEL", 'MODEL')

class NunchakuFluxDiTLoader(Node[NunchakuFluxDiTLoaderInputs, NunchakuFluxDiTLoaderOutputs]):
    """
    Original name: NunchakuFluxDiTLoader
    Category: Nunchaku
    

    Inputs:
        - model_path (str)
          The SVDQuant quantized FLUX.1 models.
        - cache_threshold (float) (default: 0)
          Adjusts the caching tolerance like `residual_diff_threshold` in WaveSpeed. Increasing the value enhances speed at the cost of quality. A typical setting is 0.12. Setting it to 0 disables the effect.
        - attention (str) (default: 'nunchaku-fp16')
          Attention implementation. The default implementation is `flash-attention2`. `nunchaku-fp16` use FP16 attention, offering ~1.2Ã— speedup. Note that 20-series GPUs can only use `nunchaku-fp16`.
        - cpu_offload (str) (default: 'auto')
          Whether to enable CPU offload for the transformer model.auto' will enable it if the GPU memory is less than 14G.
        - device_id (int) (default: 0)
          The GPU device ID to use for the model.
        - data_type (str) (default: 'bfloat16')
          Specifies the model's data type. Default is `bfloat16`. For 20-series GPUs, which do not support `bfloat16`, use `float16` instead.

    Outputs:
        - model (Model)
    """
    _original_name: str = 'NunchakuFluxDiTLoader'

    def __init__(self, model_path: str, cache_threshold: Union[float, int] = 0, attention: str = 'nunchaku-fp16', cpu_offload: str = 'auto', device_id: int = 0, data_type: str = 'bfloat16'):
        super().__init__(**{"model_path": model_path, "cache_threshold": cache_threshold, "attention": attention, "cpu_offload": cpu_offload, "device_id": device_id, "data_type": data_type})
        self.inputs = NunchakuFluxDiTLoaderInputs(self)
        self.outputs = NunchakuFluxDiTLoaderOutputs(self)
