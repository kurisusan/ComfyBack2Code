
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node

class NunchakuFluxDiTLoader(Node):
    """
    Original name: NunchakuFluxDiTLoader
    No description available.
    """
    _inputs = {
    "model_path": [
        [
            "Wan2_1-VACE_module_14B_fp8_e4m3fn.safetensors",
            "Wan2_1_VACE_1_3B_preview_bf16.safetensors",
            "cosmos_predict2_2B_video2world_480p_16fps.safetensors",
            "omnigen2_fp16.safetensors",
            "svdq-fp4_r32-flux.1-kontext-dev.safetensors",
            "wan2.1_vace_1.3B_fp16.safetensors",
            "wan2.1_vace_14B_fp8_e4m3fn.safetensors"
        ],
        {
            "tooltip": "The SVDQuant quantized FLUX.1 models."
        }
    ],
    "cache_threshold": [
        "FLOAT",
        {
            "default": 0,
            "min": 0,
            "max": 1,
            "step": 0.001,
            "tooltip": "Adjusts the caching tolerance like `residual_diff_threshold` in WaveSpeed. Increasing the value enhances speed at the cost of quality. A typical setting is 0.12. Setting it to 0 disables the effect."
        }
    ],
    "attention": [
        [
            "nunchaku-fp16",
            "flash-attention2"
        ],
        {
            "default": "nunchaku-fp16",
            "tooltip": "Attention implementation. The default implementation is `flash-attention2`. `nunchaku-fp16` use FP16 attention, offering ~1.2\u00d7 speedup. Note that 20-series GPUs can only use `nunchaku-fp16`."
        }
    ],
    "cpu_offload": [
        [
            "auto",
            "enable",
            "disable"
        ],
        {
            "default": "auto",
            "tooltip": "Whether to enable CPU offload for the transformer model.auto' will enable it if the GPU memory is less than 14G."
        }
    ],
    "device_id": [
        "INT",
        {
            "default": 0,
            "min": 0,
            "max": 0,
            "step": 1,
            "display": "number",
            "lazy": True,
            "tooltip": "The GPU device ID to use for the model."
        }
    ],
    "data_type": [
        [
            "bfloat16",
            "float16"
        ],
        {
            "default": "bfloat16",
            "tooltip": "Specifies the model's data type. Default is `bfloat16`. For 20-series GPUs, which do not support `bfloat16`, use `float16` instead."
        }
    ]
}
    _outputs = [
    "MODEL"
]
    _original_name = "NunchakuFluxDiTLoader"

    def __init__(self, model_path, cache_threshold=0, attention="nunchaku-fp16", cpu_offload="auto", device_id=0, data_type="bfloat16"):
        super().__init__(model_path=model_path, cache_threshold=cache_threshold, attention=attention, cpu_offload=cpu_offload, device_id=device_id, data_type=data_type)

    @classmethod
    def get_inputs(cls):
        return cls._inputs

    @classmethod
    def get_outputs(cls):
        return cls._outputs
