
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node, InputSlots, OutputSlots, Slot, Model, Conditioning, Latent, Image, Vae, Clip
from typing import Dict, Any, List, Union

# Define input and output slot classes for CheckpointLoaderKJ
class CheckpointLoaderKJInputs(InputSlots):
    ckpt_name: Slot[str]
    weight_dtype: Slot[str]
    compute_dtype: Slot[str]
    patch_cublaslinear: Slot[bool]
    sage_attention: Slot[str]
    enable_fp16_accumulation: Slot[bool]
    def __init__(self, node: "Node"):
        self.ckpt_name = Slot[str](node, "ckpt_name", ['v1-5-pruned-emaonly.safetensors'])
        self.weight_dtype = Slot[str](node, "weight_dtype", ['default', 'fp8_e4m3fn', 'fp8_e4m3fn_fast', 'fp8_e5m2', 'fp16', 'bf16', 'fp32'])
        self.compute_dtype = Slot[str](node, "compute_dtype", ['default', 'fp16', 'bf16', 'fp32'])
        self.patch_cublaslinear = Slot[bool](node, "patch_cublaslinear", 'BOOLEAN')
        self.sage_attention = Slot[str](node, "sage_attention", ['disabled', 'auto', 'sageattn_qk_int8_pv_fp16_cuda', 'sageattn_qk_int8_pv_fp16_triton', 'sageattn_qk_int8_pv_fp8_cuda', 'sageattn_qk_int8_pv_fp8_cuda++'])
        self.enable_fp16_accumulation = Slot[bool](node, "enable_fp16_accumulation", 'BOOLEAN')

class CheckpointLoaderKJOutputs(OutputSlots):
    model: Slot[Model]
    clip: Slot[Clip]
    vae: Slot[Vae]
    def __init__(self, node: "Node"):
        self.model = Slot[Model](node, "MODEL", 'MODEL')
        self.clip = Slot[Clip](node, "CLIP", 'CLIP')
        self.vae = Slot[Vae](node, "VAE", 'VAE')

class CheckpointLoaderKJ(Node[CheckpointLoaderKJInputs, CheckpointLoaderKJOutputs]):
    """
    Original name: CheckpointLoaderKJ
    No description available.
    """
    _original_name: str = 'CheckpointLoaderKJ'

    def __init__(self, ckpt_name: str, weight_dtype: str, compute_dtype: str = 'default', patch_cublaslinear: bool = False, sage_attention: str = 'False', enable_fp16_accumulation: bool = False):
        super().__init__(**{"ckpt_name": ckpt_name, "weight_dtype": weight_dtype, "compute_dtype": compute_dtype, "patch_cublaslinear": patch_cublaslinear, "sage_attention": sage_attention, "enable_fp16_accumulation": enable_fp16_accumulation})
        self.inputs = CheckpointLoaderKJInputs(self)
        self.outputs = CheckpointLoaderKJOutputs(self)
