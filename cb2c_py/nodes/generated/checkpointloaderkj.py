
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node

class CheckpointLoaderKJ(Node):
    """
    Original name: CheckpointLoaderKJ
    No description available.
    """
    _inputs = {
    "ckpt_name": [
        [
            "dreamshaper_8.safetensors"
        ],
        {
            "tooltip": "The name of the checkpoint (model) to load."
        }
    ],
    "weight_dtype": [
        [
            "default",
            "fp8_e4m3fn",
            "fp8_e4m3fn_fast",
            "fp8_e5m2",
            "fp16",
            "bf16",
            "fp32"
        ]
    ],
    "compute_dtype": [
        [
            "default",
            "fp16",
            "bf16",
            "fp32"
        ],
        {
            "default": "default",
            "tooltip": "The compute dtype to use for the model."
        }
    ],
    "patch_cublaslinear": [
        "BOOLEAN",
        {
            "default": False,
            "tooltip": "Enable or disable the patching, won't take effect on already loaded models!"
        }
    ],
    "sage_attention": [
        [
            "disabled",
            "auto",
            "sageattn_qk_int8_pv_fp16_cuda",
            "sageattn_qk_int8_pv_fp16_triton",
            "sageattn_qk_int8_pv_fp8_cuda",
            "sageattn_qk_int8_pv_fp8_cuda++"
        ],
        {
            "default": False,
            "tooltip": "Patch comfy attention to use sageattn."
        }
    ],
    "enable_fp16_accumulation": [
        "BOOLEAN",
        {
            "default": False,
            "tooltip": "Enable torch.backends.cuda.matmul.allow_fp16_accumulation, requires pytorch 2.7.0 nightly."
        }
    ]
}
    _outputs = [
    "MODEL",
    "CLIP",
    "VAE"
]
    _original_name = "CheckpointLoaderKJ"

    def __init__(self, ckpt_name, weight_dtype, compute_dtype="default", patch_cublaslinear=False, sage_attention=False, enable_fp16_accumulation=False):
        super().__init__(ckpt_name=ckpt_name, weight_dtype=weight_dtype, compute_dtype=compute_dtype, patch_cublaslinear=patch_cublaslinear, sage_attention=sage_attention, enable_fp16_accumulation=enable_fp16_accumulation)

    @classmethod
    def get_inputs(cls):
        return cls._inputs

    @classmethod
    def get_outputs(cls):
        return cls._outputs
