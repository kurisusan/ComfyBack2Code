
# This file is auto-generated by generate_nodes.py
# Do not edit this file directly.

from ..base_node import Node, InputSlots, OutputSlots, Slot, Model, Conditioning, Latent, Image, Vae, Clip
from typing import Dict, Any, List, Union

# Define input and output slot classes for VHS_VAEEncodeBatched
class VHS_VAEEncodeBatchedInputs(InputSlots):
    pixels: Slot[Image]
    vae: Slot[Vae]
    per_batch: Slot[int]
    def __init__(self, node: "Node"):
        self.pixels = Slot[Image](node, "pixels", 'IMAGE')
        self.vae = Slot[Vae](node, "vae", 'VAE')
        self.per_batch = Slot[int](node, "per_batch", 'INT')

class VHS_VAEEncodeBatchedOutputs(OutputSlots):
    latent: Slot[Latent]
    def __init__(self, node: "Node"):
        self.latent = Slot[Latent](node, "LATENT", 'LATENT')

class VHS_VAEEncodeBatched(Node[VHS_VAEEncodeBatchedInputs, VHS_VAEEncodeBatchedOutputs]):
    """
    Original name: VHS_VAEEncodeBatched
    Category: Video Helper Suite ðŸŽ¥ðŸ…¥ðŸ…—ðŸ…¢/batched nodes
    VAE Encode Batched ðŸŽ¥ðŸ…¥ðŸ…—ðŸ…¢<div style="font-size: 0.8em"><div id=VHS_shortdesc>Encode images as latents with a manually specified batch size.</div></div><div style="font-size: 0.8em">Some people have ran into VRAM issues when encoding or decoding large batches of images. As a workaround, this node lets you manually set a batch size when encoding images.</div><div style="font-size: 0.8em">Unless these issues have been encountered, it is simpler to use the native VAE Encode or to encode directly from a Load Video</div><div style="font-size: 0.8em"><div vhs_title="Inputs" style="display: flex; font-size: 0.8em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">Inputs: <div vhs_title="pixels" style="display: flex; font-size: 1em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">pixels: The images to be encoded.</div></div><div vhs_title="vae" style="display: flex; font-size: 1em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">vae: The VAE to use when encoding.</div></div></div></div><div vhs_title="Outputs" style="display: flex; font-size: 0.8em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">Outputs: <div vhs_title="LATENT" style="display: flex; font-size: 1em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">LATENT: The encoded latents.</div></div></div></div><div vhs_title="Widgets" style="display: flex; font-size: 0.8em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">Widgets: <div vhs_title="per_batch" style="display: flex; font-size: 1em" class="VHS_collapse"><div style="color: #AAA; height: 1.5em;">[<span style="font-family: monospace">-</span>]</div><div style="width: 100%">per_batch: The maximum number of images to encode in each batch.</div></div></div></div></div>

    Inputs:
        - pixels (Image)
        - vae (Vae)
        - per_batch (int) (default: 16)

    Outputs:
        - latent (Latent)
    """
    _original_name: str = 'VHS_VAEEncodeBatched'

    def __init__(self, pixels: Slot[Image], vae: Slot[Vae], per_batch: int = 16):
        super().__init__(**{"pixels": pixels, "vae": vae, "per_batch": per_batch})
        self.inputs = VHS_VAEEncodeBatchedInputs(self)
        self.outputs = VHS_VAEEncodeBatchedOutputs(self)
